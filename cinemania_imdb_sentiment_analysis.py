# -*- coding: utf-8 -*-
"""Cinemania_IMDB_Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-S5CmdBLNG7OzZPfg2l0PwhTXOTJni_2

# â€œCinemaniaâ€
"""

!pip install -U transformers -q

!pip install -U datasets evaluate accelerate -q

import pandas as pd
import numpy as np
from sklearn.utils import shuffle
from pathlib import Path

CSV_PATH = "IMDB Dataset.csv"   # íŒŒì¼ (Kaggle)
assert Path(CSV_PATH).exists(), "IMDB Dataset.csv ëª» ì¦ì•˜ì–´ìš”!"

df = pd.read_csv(CSV_PATH)
df = df[['review', 'sentiment']].rename(columns={'review':'text', 'sentiment':'label'})

# positiv and negativ 1000 ê°œ
pos = df[df['label']=='positive'].sample(n=1000, random_state=101)
neg = df[df['label']=='negative'].sample(n=1000, random_state=101)

final_df = shuffle(pd.concat([pos, neg], axis=0), random_state=101).reset_index(drop=True)

# Label 0/1 ì— ë³€ê²½
label_map = {'negative':0, 'positive':1}
final_df['y'] = final_df['label'].map(label_map)
final_df.head()

import re
from collections import Counter

def simple_tokenize(s: str):
    # ì†Œë¬¸ì ë³€í™˜ â†’ HTML íƒœê·¸ ì œê±° â†’ ë¬¸ì¥ë¶€í˜¸ë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ â†’ í† í° ë¦¬ìŠ¤íŠ¸ ìƒì„±
    s = re.sub(r"<.*?>", " ", s)
    s = re.sub(r"[^a-zA-Z0-9']", " ", s)
    return [t for t in s.lower().split() if t]

# 2.1 ì–´íœ˜ ì‚¬ì „ (Top 4000 + íŠ¹ìˆ˜ í† í°)
all_tokens = []
for txt in final_df['text']:
    all_tokens.extend(simple_tokenize(txt))

freq = Counter(all_tokens)
most_common = [w for w,_ in freq.most_common(4000)]
# íŠ¹ìˆ˜ í† í°
PAD_TOKEN, UNK_TOKEN = "<pad>", "<unk>"
itos = [PAD_TOKEN, UNK_TOKEN] + most_common           # index->token
stoi = {tok:i for i,tok in enumerate(itos)}          # token->index

# 2.2 í…ìŠ¤íŠ¸ë¥¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ê¸°
def encode_review(text, max_len=256):
    toks = simple_tokenize(text)
    idxs = [stoi.get(t, 1) for t in toks]            # 1 -> <unk>
    if len(idxs) >= max_len:
        return idxs[:max_len]
    return idxs + [0]*(max_len - len(idxs))          # 0 -> <pad>

final_df['input_ids'] = final_df['text'].apply(lambda s: encode_review(s, max_len=256))

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

class ReviewsDataset(Dataset):
    def __init__(self, inputs, labels):
        self.inputs = inputs
        self.labels = labels
    def __len__(self):
        return len(self.inputs)
    def __getitem__(self, i):
        x = torch.tensor(self.inputs[i], dtype=torch.long)
        y = torch.tensor(self.labels[i], dtype=torch.float32)  # BCEë¥¼ ìœ„í•œ  float
        return x, y

# Train/Test (1600/400)
SEED = 123
np.random.seed(SEED)
torch.manual_seed(SEED)

train_size = 1600
train_df = final_df.iloc[:train_size].copy()
test_df  = final_df.iloc[train_size:train_size+400].copy()

train_ds = ReviewsDataset(train_df['input_ids'].tolist(), train_df['y'].tolist())
test_ds  = ReviewsDataset(test_df['input_ids'].tolist(),  test_df['y'].tolist())

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False)

class SentimentRNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)
        self.fc  = nn.Linear(hidden_dim, 1)
        self.sig = nn.Sigmoid()
    def forward(self, x):
        # x: [B, T]
        emb = self.emb(x)                     # [B, T, E]
        out, h_n = self.rnn(emb)              # h_n: [1, B, H] â€” ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ
        logits = self.fc(h_n.squeeze(0))      # [B, 1]
        prob = self.sig(logits).squeeze(1)    # [B]
        return prob

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_rnn = SentimentRNN(vocab_size=len(itos), embedding_dim=128, hidden_dim=256).to(device)

criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model_rnn.parameters(), lr=0.001)

# 10 epoch ì½ê¸°
EPOCHS = 10
for epoch in range(1, EPOCHS+1):
    model_rnn.train()
    running = 0.0
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        prob = model_rnn(xb)
        loss = criterion(prob, yb)
        loss.backward()
        optimizer.step()
        running += loss.item()*xb.size(0)
    print(f"[RNN] Epoch {epoch:02d} | train_loss: {running/len(train_ds):.4f}")

# Test (loss + accuracy)
model_rnn.eval()
test_loss, correct, n = 0.0, 0, 0
with torch.no_grad():
    for xb, yb in test_loader:
        xb, yb = xb.to(device), yb.to(device)
        prob = model_rnn(xb)
        loss = criterion(prob, yb)
        test_loss += loss.item()*xb.size(0)
        pred = (prob >= 0.5).long()
        correct += (pred.cpu() == yb.long().cpu()).sum().item()
        n += xb.size(0)
print(f"[RNN] test_loss: {test_loss/n:.4f} | test_acc: {correct/n:.4f}")

from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split

# ì´ ë²„ì „ì€ 1600/400 ì„¤ì •ìœ¼ë¡œ ì‘ë™
train_texts, test_texts, train_labels, test_labels = train_test_split(
    final_df['text'], final_df['y'], test_size=0.20, random_state=123, stratify=final_df['y']
)

hf_train = Dataset.from_pandas(pd.DataFrame({'text':train_texts.values, 'label':train_labels.values}))
hf_test  = Dataset.from_pandas(pd.DataFrame({'text':test_texts.values,  'label':test_labels.values}))
dset = DatasetDict({'train': hf_train, 'test': hf_test})

from transformers import AutoTokenizer
tok = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def preprocess_function(batch):
    return tok(batch["text"], truncation=True, padding="max_length", max_length=256)

tokenized = dset.map(preprocess_function, batched=True, remove_columns=['text'])
tokenized = tokenized.rename_column("label", "labels")
tokenized.set_format(type='torch', columns=['input_ids','attention_mask','labels'])

!pip install -U evaluate

!pip install -U transformers -q

# ==== 5-ë‹¨ê³„: DistilBERT (legacy-safe) ====
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
import torch

# 1) Model: 2ê°œ label (neg/pos)
model_bert = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2
)

# 2) evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´ ê°„ë‹¨í•œ ì •í™•ë„ ê³„ì‚°
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    # logits -> numpy
    if isinstance(logits, torch.Tensor):
        logits = logits.detach().cpu().numpy()
    preds = np.argmax(logits, axis=-1)
    acc = (preds == labels).mean()
    return {"accuracy": float(acc)}

# 3) TrainingArguments â€” ì˜ˆì „ ë²„ì „ì—ë§Œ ì¡´ì¬í•˜ëŠ” ëª…í™•í•œ íŒŒë¼ë¯¸í„°ë“¤
args = TrainingArguments(
    output_dir="./cinemania_bert",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    logging_steps=100,
    seed=123,
    report_to="none"  # ë§ˆë¬´ ê²ƒì— log ë³´ë‚´ì§€ ì•Šì•„ìš”
)

# 4) Trainer
trainer = Trainer(
    model=model_bert,
    args=args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tok,
    compute_metrics=compute_metrics
)

# 5) í•™ìŠµê³¼ í‰ê°€
trainer.train()
eval_result = trainer.evaluate()
print("\nğŸ“Š í‰ê°€ ê²°ê³¼:", eval_result)

# --- RNN ìœ„í•œ (SentimentRNN) ---
def predict_rnn(text: str, max_length=256):
    device = next(model_rnn.parameters()).device
    model_rnn.eval()
    encoded = encode_review(text, max_len=max_length)
    x = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).to(device)
    with torch.no_grad():
        prob = model_rnn(x).item()
    label = "positive" if prob >= 0.5 else "negative"
    return {"label": label, "prob_positive": round(prob, 3)}


# --- Transformer (BERT) ìœ„í•œ ---
def predict_bert(text: str, max_length=256):
    device = next(model_bert.parameters()).device
    model_bert.eval()
    inputs = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=max_length)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        logits = model_bert(**inputs).logits
        probs = torch.softmax(logits, dim=-1).squeeze(0)
    p_pos = probs[1].item()
    label = "positive" if torch.argmax(probs).item() == 1 else "negative"
    return {"label": label, "prob_positive": round(p_pos, 3)}

samples = [
  "I had been waiting for this movie for a long time, but I didnâ€™t like it at all.",
  "The cast is amazing, and the storyline is really interesting.",
  "The visual effects of the film are good, but the story is weak."
  ]

for s in samples:
    rnn_pred = predict_rnn(s)
    bert_pred = predict_bert(s)
    print("\nğŸ¬ í…ìŠ¤íŠ¸:", s)
    print("RNN  â†’", rnn_pred)
    print("BERT â†’", bert_pred)

print("\nğŸ“Š ìµœì¢… í‰ê°€:")
print(f"RNN test_accuracy: {correct/n:.4f}")  # ë§ˆì§€ë§‰ ë¶€ë¶„ â€“ RNN ê²°ê³¼
print(f"BERT eval_accuracy: {eval_result['eval_accuracy']:.4f}")

"""## í•™ìŠµ ê³¼ì •ì˜ ì†ì‹¤ ì§€í‘œ, í‰ê°€ ê²°ê³¼, ê·¸ë¦¬ê³  ìƒˆë¡œìš´ ë¦¬ë·°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼,DistilBERT(Transformer) ëª¨ë¸ì´ RNN ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ë†’ì€ ì •í™•ë„ì™€ ë¬¸ë§¥ ì´í•´ ëŠ¥ë ¥ì„ ë³´ì˜€ë‹¤.RNN ëª¨ë¸ì€ êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ê³  í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ë‹¤ëŠ” ì¥ì ì´ ìˆì—ˆì§€ë§Œ,í…ìŠ¤íŠ¸ ë‚´ì˜ ë¯¸ë¬˜í•œ ì˜ë¯¸ë‚˜ ê°ì •ì  ë‰˜ì•™ìŠ¤ë¥¼ ì œëŒ€ë¡œ êµ¬ë¶„í•˜ì§€ ëª»í–ˆë‹¤.ë”°ë¼ì„œ Cinemania í”„ë¡œì íŠ¸ì—ì„œëŠ” Transformer ê¸°ë°˜ ëª¨ë¸ì´ ë” ì‹ ë¢°í•  ìˆ˜ ìˆê³  ë¬¸ë§¥ì— ë¯¼ê°í•œ í•´ê²°ì±…ìœ¼ë¡œ íŒë‹¨ëœë‹¤.ë˜í•œ, ìš°ì¦ˆë² í¬ì–´ ë¦¬ë·° ë¶„ì„ì—ì„œëŠ” ë‹¤ì†Œ ì˜¤ì°¨ê°€ ìˆì—ˆì§€ë§Œ, ì˜ì–´ ë¦¬ë·°ì—ì„œëŠ” ë§¤ìš° ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤."""